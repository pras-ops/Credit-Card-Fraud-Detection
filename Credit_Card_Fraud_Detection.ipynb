{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3bb5ae5d",
      "metadata": {
        "id": "3bb5ae5d"
      },
      "source": [
        "# Project Name : Credit Card Fraud Detection using PyCaret"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b82ea9b5",
      "metadata": {
        "id": "b82ea9b5"
      },
      "source": [
        "## In this notebook we will perform the following task: \n",
        "- Data Analysis\n",
        "- Feature Engineering\n",
        "- Model Building and Prediction using ML Techniques\n",
        "- Model Building and Prediction using PyCaret(Auto ML)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d3e4ec64",
      "metadata": {
        "id": "d3e4ec64"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "592275fa",
      "metadata": {
        "id": "592275fa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "import scipy\n",
        "from sklearn.metrics import classification_report,accuracy_score"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9c1b564a",
      "metadata": {
        "id": "9c1b564a"
      },
      "source": [
        "### Importing Libraries for Outlier Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8cba5c6",
      "metadata": {
        "id": "c8cba5c6"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "from sklearn.svm import OneClassSVM"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "8232cdf8",
      "metadata": {
        "id": "8232cdf8"
      },
      "source": [
        "### Reading our Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "imfvXaTenlE8",
      "metadata": {
        "id": "imfvXaTenlE8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5c307c8e",
      "metadata": {
        "id": "5c307c8e"
      },
      "outputs": [],
      "source": [
        "df= pd.read_csv(\"/content/drive/MyDrive/creditcard.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b3d444d6",
      "metadata": {
        "id": "b3d444d6"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "01c696d5",
      "metadata": {
        "id": "01c696d5"
      },
      "source": [
        "### Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "93bcfc8f",
      "metadata": {
        "id": "93bcfc8f"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ad4880ea",
      "metadata": {
        "id": "ad4880ea"
      },
      "source": [
        "#### Checking Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "be1523d3",
      "metadata": {
        "id": "be1523d3"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d90a6e15",
      "metadata": {
        "id": "d90a6e15"
      },
      "source": [
        "### Checking the distribution of Normal and Fraud cases in our Data Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d7eb9612",
      "metadata": {
        "id": "d7eb9612"
      },
      "outputs": [],
      "source": [
        "fraud_check = pd.value_counts(df['Class'], sort = True)\n",
        "fraud_check.plot(kind = 'bar', rot=0, color= 'r')\n",
        "plt.title(\"Normal and Fraud Distribution\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Frequency\")\n",
        " ## Defining labels to replace our 0 and 1 valuelabels= ['Normal','Fraud']\n",
        "## mapping those labels\n",
        "plt.xticks(range(2), labels)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ca1acce0",
      "metadata": {
        "id": "ca1acce0"
      },
      "source": [
        "\n",
        "#### Let us see what is the shape of Normal and Fraud data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "fc7b7b47",
      "metadata": {
        "id": "fc7b7b47"
      },
      "outputs": [],
      "source": [
        "fraud_people = df[df['Class']==1]\n",
        "normal_people = df[df['Class']==0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "769ac986",
      "metadata": {
        "id": "769ac986"
      },
      "outputs": [],
      "source": [
        "fraud_people.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6d6b731e",
      "metadata": {
        "id": "6d6b731e"
      },
      "outputs": [],
      "source": [
        "normal_people.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e934828b",
      "metadata": {
        "id": "e934828b"
      },
      "source": [
        "#### Finding out the avg amount in our both the data sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "22354693",
      "metadata": {
        "id": "22354693"
      },
      "outputs": [],
      "source": [
        "fraud_people['Amount'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "56718e5c",
      "metadata": {
        "id": "56718e5c"
      },
      "outputs": [],
      "source": [
        "normal_people['Amount'].describe()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "408950d4",
      "metadata": {
        "id": "408950d4"
      },
      "source": [
        "#### Let us analyse it visually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f7923628",
      "metadata": {
        "id": "f7923628"
      },
      "outputs": [],
      "source": [
        "graph, (plot1, plot2) = plt.subplots(2,1,sharex= True)\n",
        "graph.suptitle('Average amount per class')\n",
        "bins = 70\n",
        "\n",
        "plot1.hist(fraud_people['Amount'] , bins = bins)\n",
        "plot1.set_title('Fraud Amount')\n",
        "\n",
        "plot2.hist(normal_people['Amount'] , bins = bins)\n",
        "plot2.set_title('Normal Amount')\n",
        "\n",
        "plt.xlabel('Amount ($)')\n",
        "plt.ylabel('Number of Transactions')\n",
        "plt.yscale('log')\n",
        "plt.show();"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ee9fe263",
      "metadata": {
        "id": "ee9fe263"
      },
      "source": [
        "#### Plotting a corr Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "345ec37b",
      "metadata": {
        "id": "345ec37b"
      },
      "outputs": [],
      "source": [
        "df.corr()\n",
        "plt.figure(figsize=(30,30))\n",
        "g=sns.heatmap(df.corr(),annot=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d02237e8",
      "metadata": {
        "id": "d02237e8"
      },
      "source": [
        "### Creating our Dependent and Independent Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "fb9b800f",
      "metadata": {
        "id": "fb9b800f"
      },
      "outputs": [],
      "source": [
        "columns = df.columns.tolist()\n",
        "# Making our Independent Features\n",
        "columns = [var for var in columns if var not in [\"Class\"]]\n",
        "# Making our Dependent Variable\n",
        "target = \"Class\"\n",
        "x= df[columns]\n",
        "y= df[target]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "afc88598",
      "metadata": {
        "id": "afc88598"
      },
      "outputs": [],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "80dd67f5",
      "metadata": {
        "id": "80dd67f5"
      },
      "outputs": [],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "a9c4429c",
      "metadata": {
        "id": "a9c4429c"
      },
      "outputs": [],
      "source": [
        "x.head() ## Independent Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "8c881ee6",
      "metadata": {
        "id": "8c881ee6"
      },
      "outputs": [],
      "source": [
        "y.head() ## Dependent Variable"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cd8ae374",
      "metadata": {
        "id": "cd8ae374"
      },
      "source": [
        "## Model building"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a4c50a65",
      "metadata": {
        "id": "a4c50a65"
      },
      "source": [
        "### Splitting the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "87f451aa",
      "metadata": {
        "id": "87f451aa"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cd3c7275",
      "metadata": {
        "id": "cd3c7275"
      },
      "source": [
        "### We wil be using the following Models for our Anamoly Detection:\n",
        "- Isolation Forest\n",
        "- OneClassSVM"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "773bf416",
      "metadata": {
        "id": "773bf416"
      },
      "source": [
        "## Isolation Forest"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e66f45e1",
      "metadata": {
        "id": "e66f45e1"
      },
      "source": [
        "*   Isolation Forest\n",
        "   \n",
        "   Isolation Forest is an unsupervised machine learning algorithm used for anomaly detection.The algorithm is based on the concept of isolating anomalies rather than trying to model normal data points.\n",
        "\n",
        "*   Here's how the Isolation Forest algorithm works:\n",
        "\n",
        "1.  Isolation: The algorithm randomly selects a feature and a random split value within the range of that feature. It partitions the data points based on this split value, creating isolation. Anomalies are expected to require fewer partitions to be isolated compared to normal data points.\n",
        "\n",
        "2.  Recursive Partitioning: The process of recursively partitioning continues until all data points are isolated or a predefined maximum depth is reached. Each partitioning creates a binary tree-like structure called an isolation tree.\n",
        "\n",
        "3.  Anomaly Scoring: Anomaly scores are assigned to the data points based on the average path length required to isolate them. Anomalies, being isolated earlier, will have shorter average path lengths and thus higher anomaly scores.\n",
        "\n",
        "4.  Thresholding: Based on the anomaly scores, a threshold can be set to determine the cutoff point for identifying anomalies. Data points with anomaly scores above the threshold are considered anomalies.\n",
        "\n",
        "*   Isolation Forest has some advantages for anomaly detection:\n",
        "\n",
        "1.  It is effective in high-dimensional datasets, unlike some other anomaly detection algorithms that struggle with the curse of dimensionality.\n",
        "\n",
        "2.  It can handle both global and local anomalies, as it doesn't rely on assumptions about the data distribution.\n",
        "\n",
        "3.  The algorithm is computationally efficient, especially for large datasets, as it only requires a random subset of features for partitioning.\n",
        "\n",
        "4.  Isolation Forest can be applied to various domains, such as fraud detection, network intrusion detection, and outlier detection in data analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "70fdcc4c",
      "metadata": {
        "id": "70fdcc4c"
      },
      "outputs": [],
      "source": [
        "iso_forest= IsolationForest(n_estimators=100, max_samples=len(x_train),random_state=0, verbose=0)                        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "58785d19",
      "metadata": {
        "id": "58785d19"
      },
      "outputs": [],
      "source": [
        "iso_forest.fit(x_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "bcdb7fda",
      "metadata": {
        "id": "bcdb7fda"
      },
      "outputs": [],
      "source": [
        "ypred= iso_forest.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "5e0a2718",
      "metadata": {
        "id": "5e0a2718"
      },
      "outputs": [],
      "source": [
        "ypred"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c3eace63",
      "metadata": {
        "id": "c3eace63"
      },
      "source": [
        "#### Mapping the values as we want to have an output in 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "273afab3",
      "metadata": {
        "id": "273afab3"
      },
      "outputs": [],
      "source": [
        "ypred[ypred == 1] = 0\n",
        "ypred[ypred == -1] = 1\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "29473b72",
      "metadata": {
        "id": "29473b72"
      },
      "source": [
        "### Accuracy score and Matrix\n",
        "1.  Precision: Precision measures how many of the positively predicted instances are actually true positives. In other words, it focuses on the accuracy of positive predictions. It is calculated by dividing the number of true positives by the sum of true positives and false positives. A higher precision indicates fewer false positives.\n",
        "\n",
        "2.  Recall: Recall, also known as sensitivity or true positive rate, measures how many of the actual positive instances are correctly identified by the model. It focuses on the ability to find all positive instances. It is calculated by dividing the number of true positives by the sum of true positives and false negatives. A higher recall indicates fewer false negatives.\n",
        "\n",
        "3.  F1-score: The F1-score is a harmonic mean of precision and recall. It provides a single metric that combines both precision and recall into one value. The F1-score is useful when you want to consider both precision and recall simultaneously. It ranges from 0 to 1, with 1 being the best score.\n",
        "\n",
        "4.  Support: Support refers to the number of instances in each class (positive or negative) in the dataset. It provides information about the distribution of classes and can help interpret the performance of the model.\n",
        "\n",
        "In summary, precision tells us how many positive predictions are correct, recall tells us how many positive instances are correctly identified, and the F1-score provides a balanced measure by considering both precision and recall. The support value represents the number of instances in each class. These metrics are useful in assessing the performance of a classification model and understanding its strengths and weaknesses in identifying positive instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "a07329db",
      "metadata": {
        "id": "a07329db"
      },
      "outputs": [],
      "source": [
        "print(accuracy_score(y_test,ypred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "7d333120",
      "metadata": {
        "id": "7d333120"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test,ypred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "29d8b1e9",
      "metadata": {
        "id": "29d8b1e9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e856f698",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "*   A confusion matrix provides a summary of the model's predictions by \n",
        "comparing them to the actual labels in the dataset. It helps us understand how well the model is performing in terms of correctly and incorrectly classifying instances.\n",
        "*   The confusion matrix consists of four main elements:\n",
        "\n",
        "\n",
        "1.  True Positives (TP): These are the instances that are correctly predicted as positive by the model. In other words, the model correctly identified positive instances.\n",
        "\n",
        "2.  True Negatives (TN): These are the instances that are correctly predicted as negative by the model. The model correctly identified instances that are not positive.\n",
        "\n",
        "3.  False Positives (FP): These are the instances that are incorrectly predicted as positive by the model. The model mistakenly identified instances as positive when they are actually negative.\n",
        "\n",
        "4.  False Negatives (FN): These are the instances that are incorrectly predicted as negative by the model. The model failed to identify instances that are actually positive.\n",
        "\n",
        "5.  The confusion matrix is typically presented in a tabular form, with the actual labels forming the rows and the predicted labels forming the columns. It helps us visualize and analyze the performance of the model across different classes.\n",
        "\n",
        "6.  By examining the values in the confusion matrix, we can derive several evaluation metrics such as accuracy, precision, recall, and F1-score, which provide insights into the model's performance for each class.\n",
        "\n",
        "In summary, a confusion matrix is a useful tool that helps us understand how well a classification model is performing by comparing its predictions with the actual labels. It allows us to analyze the model's accuracy, as well as the rate of false positives and false negatives, providing valuable information to evaluate and improve the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "975e1a4b",
      "metadata": {
        "id": "975e1a4b"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(y_test, ypred)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "035f32f0",
      "metadata": {
        "id": "035f32f0"
      },
      "source": [
        "### We can also print how many errors our model have"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "79724950",
      "metadata": {
        "id": "79724950"
      },
      "outputs": [],
      "source": [
        "n_errors = (ypred != y_test).sum()\n",
        "print(\"Isolation Forest have {} errors.\".format(n_errors))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6adf383e",
      "metadata": {
        "id": "6adf383e"
      },
      "source": [
        "## OneClassSVM\n",
        "\n",
        "**OneClassSVM**\n",
        "OneClassSVM (Support Vector Machine) is a machine learning algorithm used for anomaly detection and novelty detection tasks. It is a type of Support Vector Machine that is trained in an unsupervised manner, meaning it doesn't require labeled data with anomalies or novelties during training.\n",
        "\n",
        "\n",
        "\n",
        "*   Here's how the OneClassSVM algorithm works:\n",
        "\n",
        "\n",
        "\n",
        "1.  Training: The OneClassSVM algorithm aims to build a decision boundary that encapsulates the majority of the data points in a high-dimensional space. It learns to distinguish between normal data points and potential outliers.\n",
        "\n",
        "2.  Support Vectors: OneClassSVM identifies support vectors, which are the data points closest to the decision boundary. These support vectors play a crucial role in defining the decision boundary and separating normal data points from anomalies.\n",
        "\n",
        "3.  Anomaly Scoring: Once the model is trained, it can assign anomaly scores to new, unseen data points. Data points that lie outside the decision boundary or have a large distance from the support vectors are considered potential anomalies and receive higher anomaly scores.\n",
        "\n",
        "4.  Thresholding: A threshold can be set on the anomaly scores to determine the cutoff point for classifying anomalies. Data points with anomaly scores above the threshold are considered anomalies.\n",
        "\n",
        "\n",
        "\n",
        "*   OneClassSVM has a few key characteristics and use cases:\n",
        "\n",
        "\n",
        "\n",
        "1.  Unsupervised Anomaly Detection: OneClassSVM is primarily used for unsupervised anomaly detection, where anomalies are detected in a dataset without explicitly labeled anomalies.\n",
        "\n",
        "2.  Non-linear Decision Boundaries: OneClassSVM is capable of learning non-linear decision boundaries through the use of kernel functions, such as the radial basis function (RBF) kernel.\n",
        "\n",
        "3.  Handling Imbalanced Data: OneClassSVM can handle imbalanced datasets where the majority of the data points are normal, and anomalies are rare.\n",
        "\n",
        "4.  Novelty Detection: OneClassSVM can also be used for novelty detection, where the goal is to identify data points that differ significantly from the training data, even if they are not necessarily anomalies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "e57b53b6",
      "metadata": {
        "id": "e57b53b6"
      },
      "outputs": [],
      "source": [
        "svm= OneClassSVM(kernel='rbf', degree=3, gamma=0.1,nu=0.05, \n",
        "                                         #max_iter=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "d1b032a7",
      "metadata": {
        "id": "d1b032a7"
      },
      "outputs": [],
      "source": [
        "svm.fit(x_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "f4cba412",
      "metadata": {
        "id": "f4cba412"
      },
      "outputs": [],
      "source": [
        "ypred1= svm.predict(x_test)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ed1c5697",
      "metadata": {
        "id": "ed1c5697"
      },
      "source": [
        "#### Here also we do the same thing as above, mapping our results in 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "29d12ee8",
      "metadata": {
        "id": "29d12ee8"
      },
      "outputs": [],
      "source": [
        "ypred1[ypred1 == 1] = 0\n",
        "ypred1[ypred1 == -1] = 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "bfc1976e",
      "metadata": {
        "id": "bfc1976e"
      },
      "source": [
        "### Accuracy score and Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "f4f2f520",
      "metadata": {
        "id": "f4f2f520"
      },
      "outputs": [],
      "source": [
        "print(accuracy_score(y_test,ypred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "1959886f",
      "metadata": {
        "id": "1959886f"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test,ypred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "555db07a",
      "metadata": {
        "id": "555db07a"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "3bbf370a",
      "metadata": {
        "id": "3bbf370a"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(y_test, ypred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "0ce625ae",
      "metadata": {
        "id": "0ce625ae"
      },
      "outputs": [],
      "source": [
        "n_errors = (ypred1 != y_test).sum()\n",
        "print(\"SVM have {} errors.\".format(n_errors))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0cedbd02",
      "metadata": {
        "id": "0cedbd02"
      },
      "source": [
        "## Solving the Problem Statement using PyCaret Library(Auto ML)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7f319094",
      "metadata": {
        "id": "7f319094"
      },
      "source": [
        "# PyCaret :"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "14f50476",
      "metadata": {
        "id": "14f50476"
      },
      "source": [
        "### PyCaret\n",
        "PyCaret is a Python library that facilitates the end-to-end machine learning workflow. It provides a simplified interface for automating various steps in the machine learning process, including data preprocessing, feature selection, model training, hyperparameter tuning, model evaluation, and deployment.\n",
        "\n",
        "  Here are some key features and benefits of PyCaret:\n",
        "\n",
        "1.  Simplified API: PyCaret offers a high-level API that abstracts away the complexities of machine learning tasks, making it easier and faster to build and deploy models.\n",
        "\n",
        "2.  Automated Preprocessing: PyCaret automates common data preprocessing tasks such as handling missing values, encoding categorical variables, feature scaling, and more.\n",
        "\n",
        "3.  Automatic Feature Selection: PyCaret can automatically select relevant features from your dataset based on various techniques such as statistical methods, importance ranking, and domain-specific feature selection algorithms.\n",
        "\n",
        "4.  Model Training and Tuning: PyCaret supports a wide range of classification, regression, clustering, and anomaly detection algorithms. It provides an easy-to-use interface for training models on your data and automatically tuning hyperparameters to optimize model performance.\n",
        "\n",
        "5.  Model Comparison and Evaluation: PyCaret allows you to compare multiple models using various evaluation metrics and visualizations, enabling you to make informed decisions about which model to choose.\n",
        "\n",
        "6.  Model Deployment: PyCaret provides functionality to deploy trained models as Python code, allowing you to integrate them into production systems or build APIs for making predictions.\n",
        "\n",
        "7.  Experiment Logging and Reproducibility: PyCaret keeps track of all the steps in your machine learning workflow, including data preprocessing, model training, and hyperparameter tuning. This makes it easy to reproduce experiments and share them with others.\n",
        "\n",
        "Overall, PyCaret aims to simplify and streamline the machine learning process, enabling users to quickly experiment with different algorithms, preprocess data efficiently, and deploy models with ease. It can be a valuable tool for data scientists, machine learning engineers, and researchers looking to accelerate their workflow and build robust machine learning models."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "883d094b",
      "metadata": {
        "id": "883d094b"
      },
      "source": [
        "### Installing Pycaret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "a35b9f64",
      "metadata": {
        "id": "a35b9f64"
      },
      "outputs": [],
      "source": [
        "!pip install pycaret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "10b9cace",
      "metadata": {
        "id": "10b9cace"
      },
      "outputs": [],
      "source": [
        "df= pd.read_csv(\"creditcard.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "bccf3855",
      "metadata": {
        "id": "bccf3855"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "859c4837",
      "metadata": {
        "id": "859c4837"
      },
      "outputs": [],
      "source": [
        "from pycaret.classification import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "3de8e738",
      "metadata": {
        "id": "3de8e738"
      },
      "outputs": [],
      "source": [
        "model= setup(data= df, target= 'Class')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "f81cc748",
      "metadata": {
        "id": "f81cc748"
      },
      "outputs": [],
      "source": [
        "compare_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "3ac8cb8c",
      "metadata": {
        "id": "3ac8cb8c"
      },
      "outputs": [],
      "source": [
        "random_forest= create_model('rf')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ada66b7f",
      "metadata": {
        "id": "ada66b7f"
      },
      "source": [
        "### As we see we have a very good Kappa score which is often seen in an Imbalanced dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "daa84895",
      "metadata": {
        "id": "daa84895"
      },
      "outputs": [],
      "source": [
        "random_forest"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "49eb360f",
      "metadata": {
        "id": "49eb360f"
      },
      "source": [
        "### We can Hypertune our model to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "b8ae8f50",
      "metadata": {
        "id": "b8ae8f50"
      },
      "outputs": [],
      "source": [
        "tuned_model= tune_model('random_forest')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2351b233",
      "metadata": {
        "id": "2351b233"
      },
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "47397b43",
      "metadata": {
        "id": "47397b43"
      },
      "outputs": [],
      "source": [
        "pred_holdout = predict_model(random_forest,data= x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "49e4d2f0",
      "metadata": {
        "id": "49e4d2f0"
      },
      "outputs": [],
      "source": [
        "pred_holdout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e84be22",
      "metadata": {
        "id": "5e84be22"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "cd3c7275",
        "14f50476"
      ],
      "name": "Credit Card Fraud Detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
